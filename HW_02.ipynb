{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 2**\n",
    "\n",
    "Due: **September 24th, 5pm** (late submission until September 27nd, 5pm -- no submission possible afterwards)\n",
    "\n",
    "Written assignment: 13 points\n",
    "\n",
    "Coding assignment: 16 points\n",
    "\n",
    "Project report: 6 points\n",
    "\n",
    "### Name: [Yawen Tan]\n",
    "\n",
    "### Link to the github repo: [https://github.com/IsabellaTan/Brown-DATA2060-HW2.git]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Written Assignment**\n",
    "\n",
    "### **Problem 1: Halfspaces** (6 points)\n",
    "\n",
    "Let $X=\\{0,1\\}_d$ and $Y=\\{-1,1\\}$. Consider the example below and answer the questions:\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Output is 1 if and only if $x_1$ is 0 (negation of $x_1$ or $\\neg x_1$).\n",
    "\n",
    "- Labeling function: \n",
    "$$f(\\textbf{x}) = \\neg x_1$$\n",
    "- One possible halfspace *h:* $X \\rightarrow Y$: \n",
    "$$h_{\\textbf{w}}(\\textbf{x}) = \\text{sign}(-x_1 + 1/2)$$\n",
    "- where the weight vector is w = (-1, 0, 0, ..., 0) and the bias term is 1/2.\n",
    "\n",
    "- A graphical representation is shown below:\n",
    "<center><img src=\"prob1_neg_figure.png\" width=\"300\"></center>\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "Define a halfspace *h:* $X \\rightarrow Y$ for the following functions:\n",
    "1.  **Conjunction:** Output is 1 if and only if all *d* attributes are 1.\n",
    "\n",
    "2.  **Majority:** Output is 1 if and only if more than half of the *d*\n",
    "    attributes are 1.\n",
    "\n",
    "Make sure to explain why you have chosen your weights and why they exhibit the desired behavior. You can assume that there is a bias term and that you can express your weights and bias in terms of *d*. \n",
    "\n",
    "*Note:* The sign function can be considered as\n",
    "\n",
    "sign $(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\\\\\ 0 & \\text{if } x = 0 \\text{ (Should not happen to achieve 100\\% accuracy)} \\\\\\\\ -1 & \\text{if } x < 0 \\end{cases}$\n",
    "\n",
    "for all problems on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "1. \n",
    "\n",
    "Define $h(x) = w_1 x_1 + w_2 x_2 + w_3 x_3 +....+ w_d x_d + b$\n",
    "\n",
    "Let $w_i$ = 1 for i = 1,2,...,d. \n",
    "\n",
    "Because we would like to determine when all d attributes are 1, output is 1, then when w=1, the sum of all d attributes are d; otherwise the sum of all d attributes are less than d. \n",
    "\n",
    "Let b = -d + 0.5. \n",
    "\n",
    "Because when all d attibutes are 1, the sum of xi is d, and we want the output 1 if and only if sum of $x_i$ is d, so that d + b > 0 and d - 1 + b < 0. Then -d < b < -d + 1. We choose b = -d + 0.5\n",
    "\n",
    "Thus, $w_1,...,w_d$ = 1，and b = -d + 0.5, $h(x) = sign(x_1 + x_2 + ... + x_d - d + 0.5)$\n",
    "\n",
    "\n",
    "2. \n",
    "\n",
    "Define $h(x) = w_1 x_1 + w_2 x_2 + w_3 x_3 +....+ w_d x_d + b$\n",
    "\n",
    "Let $w_i$ = 1 for i = 1,2,...,d. \n",
    "\n",
    "Because we would like to determine when more than half d attributes are 1, output is 1, then when w=1, the sum of more than half d attributes are greater than d/2; otherwise the sum of more than half d attributes are less than or equal to d/2. \n",
    "\n",
    "And we can express sum of more than half d attributes are greater than d/2 as the sum is greater or equal to ⌊d/2⌋ + 1 (because we need to consider if d is odd or even).\n",
    "\n",
    "Let b = -⌊d/2⌋ - 0.5 \n",
    "\n",
    "Because the output is 1 if and only if sum of $x_i$ is greater or equal to ⌊d/2⌋ + 1, so that ⌊d/2⌋ + 1 + b > 0 and ⌊d/2⌋ + b < 0. Then -⌊d/2⌋ - 1 < b < -⌊d/2⌋. We choose b= -⌊d/2⌋ - 0.5 \n",
    "\n",
    "Thus, $w_1,...,w_d$ = 1，and b = -⌊d/2⌋ - 0.5, $h(x) = sign(x_1 + x_2 + ... + x_d -⌊d/2⌋ - 0.5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 2: More Halfspaces** (3 points)\n",
    "\n",
    "Consider the example below and answer the question:\n",
    "\n",
    "**Example:**\n",
    "- Here is the proof that XOR (exclusive or),  $f^C(\\textbf{x}) = x_1 \\text{ xor } x_2$, cannot be represented with a halfspace. You can use a similar argument for this problem.\n",
    "$$h_{\\textbf{w}}(\\textbf{x}) = \\text{sign}(w_1 x_1 + w_2 x_2 + b)$$\n",
    "- When $x_1$ and $x_2$ are both 0, exclusive or should be false (negative sign):\n",
    "$$(0, 0) : b < 0 \\implies -b > 0$$\n",
    "- When exactly one of $x_1$ or $x_2$ is 1, then exclusive or should be true (positive sign):\n",
    "$$(1, 0) : w_1 + b > 0$$\n",
    "$$(0,1) : w_2 + b > 0$$\n",
    "- When $x_1$ and $x_2$ are both 1, exclusive or should be false (negative sign):\n",
    "$$(1,1) : w_1 + w_2 + b < 0$$\n",
    "- The sum of the first three constraints gives: $w_1 + w_2 + b > 0$, which contradicts with the rule for $(1,1)$! So we know XOR cannot be represented as a halfspace. A graphical representation is shown below:\n",
    "<center><img src=\"prob2_XOR_figure.png\" width=\"300\"></center>\n",
    "\n",
    "**Question:**\n",
    "\n",
    "Consider the function $h_{equiv}:\\{0,1\\}^2 \\rightarrow \\{-1,1\\}$ (ie. $x_1, x_2 \\in \\{0,1\\}$) defined as\n",
    "$$h_{equiv}(x_1,x_2) = \\begin{cases}\n",
    "      1 & \\text{ if } x_1 = x_2,\\\\\\\\\n",
    "      -1 & \\text {otherwise.}\n",
    "  \\end{cases}$$\n",
    "Show that *h*<sub>equiv</sub> cannot be represented as a halfspace with\n",
    "a bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $h_{equiv} = \\text{sign}(w_1 x_1 + w_2 x_2 + b)$\n",
    "\n",
    "When $x_1$ and $x_2$ are both 0, function should be true (positive sign):\n",
    "$$(0, 0) : b > 0 \\implies -b < 0$$\n",
    "When exactly one of $x_1$ or $x_2$ is 1, then function should be false (negative sign):\n",
    "$$(1, 0) : w_1 + b < 0$$\n",
    "$$(0,1) : w_2 + b < 0$$\n",
    "When $x_1$ and $x_2$ are both 1, function should be true (positive sign):\n",
    "$$(1,1) : w_1 + w_2 + b > 0$$\n",
    "The sum of the first three constraints gives: $w_1 + w_2 + b < 0$, which contradicts with the rule for $(1,1)$. So we know $h_{equiv}$ cannot be represented as a halfspace with a bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem 3: Decision Boundaries** (4 points)\n",
    " \n",
    "  \n",
    "Consider an arbitrary halfspace classifier of the form\n",
    "$h_{\\bf w}({\\bf x}) = \\text{sign}(\\langle {\\bf w}, {\\bf x} \\rangle)$\n",
    "where $h: \\mathbb{R}^n \\rightarrow \\{-1,1\\}, w,x \\in \\mathbb{R}^n$, and\n",
    "there is no bias term. Prove that the distance from an arbitrary example\n",
    "${\\bf x}$ to the decision boundary defined by ${\\bf w}$ is:\n",
    "$$\\frac{|\\langle{\\bf w}, {\\bf x} \\rangle|}{||\\textbf{w}||_2}$$\n",
    "\n",
    "The distance from an example ${\\bf x}$ to the decision boundary is defined as the distance from the example to the point ${\\bf a}$ on the decision boundary with minimum distance to the example ${\\bf x}$. $\\|{\\bf w}\\|_2$ denotes the *L*<sup>2</sup> norm, given by $||\\textbf{w}||_2 = \\sqrt{w_1^2 + w_2^2 + ... + w_n^2}$.  \n",
    "  \n",
    "  \n",
    "*Reminder:* The decision boundary of a halfspace classifier is the set\n",
    "of points in $X$ where the classifier's output changes from -1 to 1, i.e.,\n",
    "the solution to $\\langle{\\bf w}, {\\bf x}\\rangle = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the decision boundary be $H = {z ∈ R^n : ⟨w,z⟩=0}$.\n",
    "\n",
    "Let $a ∈ H$ be the point on the decision boundary that is closest to x. \n",
    "\n",
    "The vector from a to x, which is x−a, must be orthogonal to the decision boundary.  So it is parallel to the normal vector w (because ⟨w,z⟩=0). Then there exists a scalar $λ ∈ R$ such that $x−a = λw$. So $a = x−λw$.\n",
    "\n",
    "Because a lies on decision boundary H, it satisfies ⟨w,a⟩=0. Then we substitute $a = x−λw$ and get $⟨w,x−λw⟩ = 0$ \n",
    "\n",
    "Since $⟨w,x−λw⟩ = 0$, then $⟨w,x⟩−λ⟨w,w⟩=0$, so that $λ = \\frac{<w,x>}{<w,w>} = \\frac{<w,x>}{||w||_2 ^2}$.\n",
    "\n",
    "The distance d from x to the boundary decision H is $d = ||x−a||_2$.\n",
    "\n",
    "Since $x−a = λw$, then $d = ||x−a||_2 = ||λw||_2 = |λ| ||w||_2$\n",
    "\n",
    "We substitute $λ = \\frac{<w,x>}{||w||_2 ^2}$ into $d = |λ| ||w||_2$, then we can get $d = |\\frac{<w,x>}{||w||_2 ^2}|  ||w||_2$. After simplify, $d = |\\frac{<w,x>}{||w||_2}|$\n",
    "\n",
    "Thus, we have proved the distance from an arbitrary example ${\\bf x}$ to the decision boundary defined by ${\\bf w}$ is: $\\frac{|\\langle{\\bf w}, {\\bf x} \\rangle|}{||\\textbf{w}||_2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Coding Assignment** (16 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the environment test below, make sure you get all green check, if not, you will lose 2 points for each red flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.12.11\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.10.5 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.7.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.3.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pytest version 8.4.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m torch version 2.7.1 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.11\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.11\"):\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.10.5\", 'numpy': \"2.3.2\",'sklearn': \"1.7.1\", \n",
    "                'pandas': \"2.3.2\", 'pytest': \"8.4.1\", 'torch':\"2.7.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "After months of studying to become a pastry chef, Andras decided to take\n",
    "some time off and host a wine night for his friends. \n",
    "Unfortunately, with all the time it takes to make desserts, he\n",
    "hasn't had time to become a wine connoisseur, so he's asked you to help\n",
    "him choose which wines to stock, with the help of machine learning!  \n",
    "  \n",
    "From his group of friends, we've collected quite a lot of data, and need\n",
    "your help to electronically determine how each person rated a wine on a\n",
    "scale of 1 to 10.  \n",
    "  \n",
    "In this assignment, you'll implement linear regression and use your\n",
    "model to predict wine quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stencil Code & Data\n",
    "  \n",
    "We have provided the following stencil code within notebok:\n",
    "\n",
    "-   `Models` contains the `Linear Regression` model you will be\n",
    "    implementing.\n",
    "\n",
    "- `Check Model` contains a series of tests to ensure you are coding your \n",
    "    model properly.\n",
    "\n",
    "-   `Main` is the entry point of program which will read in the\n",
    "    dataset, run the model, and print the results.\n",
    "\n",
    "You should not modify any code in the `main`. If you do for debugging\n",
    "or other purposes, please make sure any additions are commented out in\n",
    "the final handin. The autograder will run on an unmodified version of\n",
    "`main`. All the functions you need to fill in reside in this notebook,\n",
    "marked by `TODO`s. You can see a full description of them in the section\n",
    "below.\n",
    "\n",
    "#### Datasets : UCI Wine Quality\n",
    "\n",
    "For the Linear Regression model, you will be using the UCI Wine Quality\n",
    "Dataset, which contains information about various attributes of a\n",
    "wine and its corresponding quality rating (out of 10). It includes 4898\n",
    "examples, which will be split into training and testing datasets using\n",
    "the `sklearn` library (you do not have to worry about this, as we have\n",
    "implemented it for you - you will be doing this in the next\n",
    "assignment!). Each example contains 12 attributes, and your model will\n",
    "train on the first 11 attributes (and a 12th bias term) to predict the\n",
    "last value. More information about the dataset can be found at\n",
    "<https://archive.ics.uci.edu/ml/datasets/Wine+Quality>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Model`, there are three functions you will implement. They are:\n",
    "\n",
    "-   `squared_error()` is a helper function that calculates the sum\n",
    "squared difference between two arrays.\n",
    "\n",
    "-   `train()` uses matrix inversion to find the optimal set\n",
    "    of weights for the given data.\n",
    "\n",
    "-   `predict()` predicts the values of test data points using the\n",
    "    trained weights.\n",
    "\n",
    "    In addition, three methods are provided for you. You should not\n",
    "    change them.\n",
    "\n",
    "    -   `loss()` computes the squared error loss of the predicted labels\n",
    "        over a dataset.\n",
    "\n",
    "    -   `average_loss()` computes the average squared error loss per\n",
    "        prediction over a dataset.\n",
    "\n",
    "*Note*: You are not allowed to use any off-the-shelf packages that have\n",
    "already implemented these models, such as scikit-learn or any linear\n",
    "regression functions. We're asking you to implement them yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Linear Regression**\n",
    "\n",
    "Linear Regression learns linear functions of the inputs:\n",
    "$$h_{\\bf w}(\\bf{x}) = \\langle {\\bf w} , {\\bf x} \\rangle$$\n",
    "Note: the bias term can be included here by padding the data with an\n",
    "extra feature of 1. This has been already done for you in the stencil.  \n",
    "  \n",
    "As we are using squared loss, the ERM hypothesis has weights\n",
    "$${\\bf w} = \\text{argmin}_{\\bf w} \\sum_{i = 1}^{m}(y_{i} - h_{\\bf w}({{\\bf x}_i}))^{2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Squared Loss**\n",
    "\n",
    "For this assignment, we will be evaluating and training the Linear\n",
    "Regression model using sum squared loss (or L2 loss). Recall that the L2\n",
    "loss function is defined as:\n",
    "$$L_S(h_{\\bf w}) = \\sum\\limits_{i=1}^m(y_{i}-h_{\\bf w}({\\bf x}_{i}))^{2}$$\n",
    "where *y*<sub>i</sub> is the target value of *i*<sup>th</sup>\n",
    "sample and $h_{\\bf w}({\\bf x}_{i})$ is the predicted value of that\n",
    "sample given the learned model weights. Your model should seek to\n",
    "minimize this loss through matrix inversion to learn the ERM hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Matrix Inversion**\n",
    "\n",
    "You can assume that the examples in the training data are linearly\n",
    "independent. In that case, we showed in lecture that you can use matrix\n",
    "inversion to compute the vector of weights $\\bf{w}$ that minimizes the\n",
    "squared loss. The equation to find **w**, for a set of data points *X*\n",
    "and their labels ${\\bf y}$ is\n",
    "$${\\bf w} = (X^T X)^{-1} X^T {\\bf y}$$\n",
    "Note: *X* here is a matrix of examples stacked row-wise (i.e.\n",
    "${\\bf x_1}$ is the first row of *X* and so on). In lecture we saw that\n",
    "with *X* as a matrix of examples stacked column-wise (i.e. ${\\bf x_1}$\n",
    "is the first column of *X* and so on), the equation is equivalently:\n",
    "$${\\bf w} = A^{-1}{\\bf b} = (XX^T)^{-1}X{\\bf y}$$\n",
    "Implement the solution for ${\\bf w}$ in `LinearRegression`, using the\n",
    "`np.linalg.pinv` function to calculate matrix inverses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def squared_error(predictions, Y):\n",
    "    '''\n",
    "    Computes sum squared loss (the L2 loss) between true values, Y, \n",
    "    and predictions.\n",
    "    @params:\n",
    "        Y: A 1D Numpy array with real values (float64)\n",
    "        predictions: A 1D Numpy array of the same size of Y\n",
    "    @return:\n",
    "        sum squared loss (the L2 loss) using predictions for Y.\n",
    "    '''\n",
    "    # [TODO]\n",
    "    # Compute the error\n",
    "    error = Y - predictions\n",
    "    # Square the error\n",
    "    squared_error = error ** 2\n",
    "    # Return the sum of squared errors\n",
    "    return np.sum(squared_error)\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    '''\n",
    "    LinearRegression model that minimizes squared error using matrix inversion.\n",
    "    '''\n",
    "    def __init__(self, n_features):\n",
    "        '''\n",
    "        @attrs:\n",
    "            n_features: the number of features in the regression problem\n",
    "            weights: The weights of the linear regression model.\n",
    "        '''\n",
    "        # An extra feature added for the bias value\n",
    "        self.n_features = n_features + 1  \n",
    "        self.weights = np.zeros(n_features + 1)\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Trains the LinearRegression model by finding the optimal set of weights\n",
    "        using matrix inversion.\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, \n",
    "            padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding values \n",
    "            for each example\n",
    "        @return:\n",
    "            the weights of the regression model\n",
    "        '''\n",
    "        # [TODO]\n",
    "        # Compute X^T X\n",
    "        XtX = np.dot(X.T, X)\n",
    "        # Compute X^T Y\n",
    "        XtY = np.dot(X.T, Y)\n",
    "        # Compute weight w = (X^T X)^(-1) X^T Y\n",
    "        self.weights = np.dot(np.linalg.pinv(XtX), XtY)\n",
    "        return self.weights\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Returns predictions of the model on a set of examples X.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, \n",
    "            padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X \n",
    "            containing the predicted value.\n",
    "        '''\n",
    "        # [TODO]\n",
    "        return np.dot(X, self.weights)\n",
    "\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the total squared error on some dataset (X, Y).\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, \n",
    "            padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding values \n",
    "            for each example\n",
    "        @return:\n",
    "            A float number which is the squared error of the model \n",
    "            on the dataset\n",
    "        '''\n",
    "        predictions = self.predict(X)\n",
    "        return squared_error(predictions, Y)\n",
    "\n",
    "\n",
    "    def average_loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the mean squared error on some dataset (X, Y).\n",
    "        MSE = Total squared error/# of examples\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, \n",
    "            padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding values \n",
    "            for each example\n",
    "        @return:\n",
    "            A float number which is the mean squared error of \n",
    "            the model on the dataset\n",
    "        '''\n",
    "        return self.loss(X, Y)/X.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "# Test Squared Error\n",
    "assert squared_error(np.array([0]),np.array([0])) == 0\n",
    "assert squared_error(np.array([2,-2,3,1]),np.array([1,1,2,2])) == 12\n",
    "\n",
    "# Create Test Model\n",
    "test_model1 = LinearRegression(2)\n",
    "x1 = np.array([[1,0],[1,2],[2,3]])\n",
    "y1 = np.array([1,5,8])\n",
    "\n",
    "x2 = np.array([[1, 2], [1, 3], [1, 4]])\n",
    "y2 = np.array([3, 5, 7])\n",
    "test_model2 = LinearRegression(2)\n",
    "\n",
    "# Tests train\n",
    "assert test_model1.train(x1, y1) == pytest.approx(np.array([1, 2]))\n",
    "assert test_model2.train(x2, y2) ==  pytest.approx(np.array([-1, 2]))\n",
    "\n",
    "# Test predict\n",
    "assert test_model1.predict(np.array([[0,1], [-1,.5], [1,2]])) == pytest.approx(\n",
    "    np.array([2,0,5]))\n",
    "assert test_model2.predict(np.array([[0,1], [-1,.5], [1,2]])) == pytest.approx(\n",
    "    np.array([2,2,3]))\n",
    "\n",
    "# Test average_loss\n",
    "assert test_model1.average_loss(np.array([[0,1], [-1,.5], [1,2]]), \n",
    "                                test_model1.predict(np.array(\n",
    "                                    [[0,1], [-1,.5], [1,2]]))) == pytest.approx(\n",
    "                                        0)\n",
    "assert test_model2.average_loss(np.array([[0,1], [-1,.5], [1,2]]), \n",
    "                                test_model2.predict(np.array(\n",
    "                                    [[0,1], [-1,.5], [1,2]]))) == pytest.approx(\n",
    "                                        0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read sklearn.datasets.make_regression data - test case 3\n",
    "X = pd.read_csv(\"./test_X.csv\", header=None).to_numpy()\n",
    "y = pd.read_csv(\"./test_y.csv\", header=None).to_numpy()\n",
    "test_model = LinearRegression(4)\n",
    "# Test train\n",
    "assert test_model.train(X, y) == pytest.approx(\n",
    "    np.array([[89.88],[77.64],[97.00],[94.30]]), rel=0.01)\n",
    "# Test predict\n",
    "assert test_model.predict(X) == pytest.approx(\n",
    "    np.array([[177.00],[-286.41],[-88.97],[-338.27],[2.95],\\\n",
    "    [28.82],[-118.96],[-239.44],[235.41],[-115.59]]), rel=0.01)\n",
    "# Test Squared Error\n",
    "assert squared_error(test_model.predict(X), y) == pytest.approx(3.58, rel=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- LINEAR REGRESSION w/ Matrix Inversion ---\n",
      "Average Training Loss: 0.5403729394828255\n",
      "Average Testing Loss: 0.6598453517957841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "WINE_FILE_PATH = 'wine.txt'\n",
    "\n",
    "def import_wine(filepath, test_size=0.2):\n",
    "    '''\n",
    "        Helper function to import the wine dataset\n",
    "        @param:\n",
    "            filepath: path to wine.txt\n",
    "            test_size: the fraction of the dataset set aside for testing\n",
    "        @return:\n",
    "            X_train: training data inputs\n",
    "            Y_train: training data values\n",
    "            X_test: testing data inputs\n",
    "            Y_test: testing data values\n",
    "    '''\n",
    "\n",
    "    # Load in the dataset\n",
    "    data = np.loadtxt(filepath, skiprows=1)\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    # Normalize the inputs\n",
    "    X = (X-np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X, Y, test_size=test_size)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "def test_linreg():\n",
    "    '''\n",
    "        Helper function that tests LinearRegression.\n",
    "    '''\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = import_wine(WINE_FILE_PATH)\n",
    "\n",
    "    num_features = X_train.shape[1]\n",
    "\n",
    "    # Padding the inputs with a bias\n",
    "    X_train_b = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_test_b = np.append(X_test, np.ones((len(X_test), 1)), axis=1)\n",
    "\n",
    "    #### Matrix Inversion ######\n",
    "    print('---- LINEAR REGRESSION w/ Matrix Inversion ---')\n",
    "    solver_model = LinearRegression(num_features)\n",
    "    solver_model.train(X_train_b, Y_train)\n",
    "    print('Average Training Loss:', \n",
    "          solver_model.average_loss(X_train_b, Y_train))\n",
    "    print('Average Testing Loss:', \n",
    "          solver_model.average_loss(X_test_b, Y_test))\n",
    "\n",
    "\n",
    "# Set random seeds. DO NOT CHANGE THIS IN YOUR FINAL SUBMISSION.\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "test_linreg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Report** (6 points)\n",
    "\n",
    "Each programming assignment in this course will be accompanied by a\n",
    "short report in which you will answer a few guiding questions. These\n",
    "questions are included to promote critical thinking about the results of\n",
    "your algorithms, both mathematically and societally. By the end of this\n",
    "course you will not only be able to implement common machine-learning\n",
    "algorithms but also develop intuition as to how the results of a given\n",
    "algorithm should be interpreted and can impact the world around you.    \n",
    "\n",
    "### **Question 1**\n",
    "\n",
    "Linear regression analysis makes several assumptions. For one, all\n",
    "observations in the data must be independent of each other (e.g.,\n",
    "the data should not include more than one observation on any\n",
    "individual/unit). Furthermore, the data should avoid including\n",
    "extreme values since these will skew the results and create a false\n",
    "sense of relationship in the data. In general, linear regression\n",
    "gives more weight to cases that are far from the average. Can you\n",
    "think of any examples or datasets in which this might pose an issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Example 1: Multiple measurements of blood pressure or test results for the same set of patients over time.This dataset is non-iid because each patient’s results come from different distributions.Using linear regression directly would incorrectly treat each observation as independent, leading to biased estimates.\n",
    "\n",
    "Example 2: Product prices may spike or drop sharply during holidays or promotional events. These extreme values can disproportionately influence the linear regression model, causing the overall predictions to deviate from the true trend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "\n",
    "Suppose a machine learning researcher at a company learns a model to\n",
    "automate the hiring process. This company sells software based on\n",
    "this model to other companies looking to expedite their hiring.\n",
    "However, it is later discovered that the algorithm heavily favors\n",
    "members of a certain class unfairly.\n",
    "\n",
    "1.  In this situation, who should be to blame for the unfair hiring?\n",
    "    \n",
    "2.  On whom does the responsibility fall to check the fairness of\n",
    "    automated systems?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1.  Machine learning researcher: When the researcher collect and clean data, if the collected data is biased (e.g., only includes certain groups) or improperly processed, the algorithm will learn these biases.\n",
    "\n",
    "    Company which sells software: Must test the algorithm for fairness before distributing it; failure to do so incurs responsibility.\n",
    "\n",
    "2.  Machine learning researcher: Must ensure the algorithm treats all groups fairly and does not introduce systematic bias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
